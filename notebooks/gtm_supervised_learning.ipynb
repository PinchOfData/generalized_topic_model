{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5550a6c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/germain/Documents/topic models/generalized_topic_model/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/germain/Documents/topic models/generalized_topic_model/notebooks/../gtm/corpus.py:77: RuntimeWarning: divide by zero encountered in log\n",
      "  self.log_word_frequencies = torch.FloatTensor(np.log(np.array(self.M_bow.sum(axis=0)).flatten()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1\tTraining Loss:2.0650804\n",
      "Epoch   1\tValidation Loss:1.6289113\n",
      "Epoch   2\tTraining Loss:0.7424154\n",
      "Epoch   2\tValidation Loss:1.4669273\n",
      "Epoch   3\tTraining Loss:0.6949086\n",
      "Epoch   3\tValidation Loss:1.5437074\n",
      "Epoch   4\tTraining Loss:0.6834591\n",
      "Epoch   4\tValidation Loss:1.6333249\n",
      "Epoch   5\tTraining Loss:0.6769996\n",
      "Epoch   5\tValidation Loss:1.8549132\n",
      "Epoch   6\tTraining Loss:0.6572475\n",
      "Epoch   6\tValidation Loss:1.9579830\n",
      "Epoch   7\tTraining Loss:0.6448019\n",
      "Epoch   7\tValidation Loss:2.0447390\n",
      "Epoch   8\tTraining Loss:0.6509201\n",
      "Epoch   8\tValidation Loss:2.1071834\n",
      "Epoch   9\tTraining Loss:0.6435065\n",
      "Epoch   9\tValidation Loss:2.2444357\n",
      "Epoch  10\tTraining Loss:0.6502141\n",
      "Epoch  10\tValidation Loss:2.0692913\n",
      "['amendment', 'hon', 'clause', 'say', 'law', 'debate', 'make', 'member']\n",
      "['hon', 'year', 'people', 'work', 'say', 'make', 'service', 'time']\n",
      "['hon', 'school', 'work', 'people', 'year', 'teacher', 'friend', 'education']\n",
      "['hon', 'year', 'people', 'work', 'make', 'say', 'time', 'friend']\n",
      "['hon', 'people', 'make', 'say', 'year', 'work', 'take', 'time']\n",
      "['amendment', 'no', 'subsection', 'clause', 'page', 'law', 'insert', 'section']\n",
      "['hon', 'say', 'make', 'people', 'year', 'member', 'time', 'give']\n",
      "['hon', 'people', 'make', 'year', 'say', 'work', 'time', 'friend']\n",
      "['school', 'apprenticeship', 'teacher', 'pupil', 'training', 'apprentice', 'college', 'prison']\n",
      "['amendment', 'hon', 'clause', 'say', 'member', 'power', 'make', 'right']\n",
      "['hon', 'make', 'say', 'people', 'friend', 'year', 'take', 'need']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/hansard_speeches_processed.csv')\n",
    "df = df.sample(n=100000, random_state = 42)\n",
    "\n",
    "train = df.sample(frac=0.5,random_state = 42)\n",
    "test = df.drop(train.index).reset_index(drop=True)\n",
    "train = train.reset_index(drop=True)\n",
    "\n",
    "import sys\n",
    "sys.path.append('../gtm/')\n",
    "from corpus import GTMCorpus\n",
    "from gtm import GTM\n",
    "\n",
    "# Create a GTMCorpus objects (one train and one test set to avoid overfitting the supervised learning algorithm)\n",
    "train_dataset = GTMCorpus(\n",
    "    train, # Must contain a column 'doc' with the text of each document and a column 'doc_clean' with the cleaned text of each document.\n",
    "    labels = \"~ party\", # The features to predict. Would be \"~ gdp\" if the df has a column 'gdp'.\n",
    "    content='~ 1' # To absorb frequent/procedural words\n",
    ")\n",
    "\n",
    "test_dataset = GTMCorpus(\n",
    "    test, # Must contain a column 'doc' with the text of each document and a column 'doc_clean' with the cleaned text of each document.\n",
    "    labels = \"~ party\", # The features to predict. Would be \"~ gdp\" if the df has a column 'gdp'.\n",
    "    content='~ 1', # To absorb frequent/procedural words,\n",
    "    vectorizer = train_dataset.vectorizer # pass on the same vectorizer as for the training set (this ensures the document term matrices have the same number of dimensions)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "tm = GTM(\n",
    "    train_dataset, \n",
    "    test_dataset,\n",
    "    n_topics = 10,\n",
    "    doc_topic_prior = 'dirichlet', # other option is \"logistic_normal\"\n",
    "    update_prior = False, # no prevalence covariates so no need to update the prior\n",
    "    predictor_type = 'classifier', # 'regressor' for continuous variables such as GDP\n",
    "    num_epochs = 10, # No need to run many epochs. I found 10 to work well on 50 000 speeches.\n",
    "    w_pred_loss = 1, # how much weight should we give to the prediction task in the likelihood?\n",
    "    print_every = 100, # print progress every x batches\n",
    "    log_every = 10, # print topic-word dist every x epochs\n",
    "    batch_size=1024,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
